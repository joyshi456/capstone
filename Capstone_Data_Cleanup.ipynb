{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJBAeTFLWb0w",
        "outputId": "66a18991-07ce-49aa-e512-4d26820a29f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports:"
      ],
      "metadata": {
        "id": "ShpdtNZ2WiCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "Lpp-aFHtWnfH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Excel file\n",
        "\n"
      ],
      "metadata": {
        "id": "j9i8xUq4WskV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "QplA8Gj6WtT6",
        "outputId": "976dc883-9efe-447c-d5f4-5a88836cb790"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d93ecf9c-bb77-4f19-8185-8a41ba4db10b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d93ecf9c-bb77-4f19-8185-8a41ba4db10b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up environment (read the file, create output folder)\n"
      ],
      "metadata": {
        "id": "SfWmqWABXJnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excel_filename = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "LH9DIRVUXWZs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(excel_filename, dtype={'Page ID': str})"
      ],
      "metadata": {
        "id": "dbdhFVfXXYIM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = \"com_links\"\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "to0ogsGbXa_L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "print(\"Column Names Detected:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM0aD78zX_nc",
        "outputId": "aba98059-3dca-43b5-f600-193d2dc62c24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names Detected:\n",
            "['URL', 'Result Block Title', 'Result Block Author', 'Result Block Text', 'Details', 'Page ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save information from the excel file into separate json files for data processing later."
      ],
      "metadata": {
        "id": "zKK-1VV7XcQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    page_id = str(row['Page ID']).strip()\n",
        "\n",
        "    # Prepare dictionary: convert datetime and other non-serializable objects to strings\n",
        "    json_data = {\n",
        "        \"url\": str(row['URL']),\n",
        "        \"page_id\": str(row['Page ID']),\n",
        "        \"result_block_title\": str(row['Result Block Title']),\n",
        "        \"result_block_author\": str(row['Result Block Author']),\n",
        "        \"result_block_text\": str(row['Result Block Text']),\n",
        "        \"details\": str(row['Details'])\n",
        "    }\n",
        "\n",
        "    # Save JSON\n",
        "    json_filename = os.path.join(output_folder, f\"{page_id}.json\")\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"✅ All JSON files saved in '{output_folder}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr5Q6f19Xk01",
        "outputId": "1b7da85f-c06f-4e24-f377-0aab42dc5d16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All JSON files saved in 'com_links'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Page Rank Data"
      ],
      "metadata": {
        "id": "mxjPe43JgHcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The jsons do not have the Page Rank data that I collected. This following section adds the page rank data from the search results. 0 means Featured Snippet"
      ],
      "metadata": {
        "id": "BWmPIQUvf7HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "uploaded = files.upload()\n",
        "txt_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "print(f\"Uploaded: {txt_file_name}\")\n",
        "\n",
        "id_to_ranks = {}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "_-93aWEDgGFr",
        "outputId": "3b7723d2-a39e-40b3-a7c3-8258097e3c4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41affca8-89bf-42a5-bea5-6297423442e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41affca8-89bf-42a5-bea5-6297423442e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving page_rankings.txt to page_rankings.txt\n",
            "Uploaded: page_rankings.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse the Txt File"
      ],
      "metadata": {
        "id": "YIXUCA5vhLwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(txt_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Each line format: \"<WEBPAGE_ID> <RANK> <SEARCH_TERM>\"\n",
        "        parts = line.split(\" \", 2)\n",
        "        if len(parts) < 3:\n",
        "            continue  # skip malformed lines\n",
        "        webpage_id, rank_value, search_term = parts[0], parts[1], parts[2]\n",
        "        entry = {\n",
        "            \"search_query\": search_term,\n",
        "            \"rank\": rank_value\n",
        "        }\n",
        "        if webpage_id not in id_to_ranks:\n",
        "            id_to_ranks[webpage_id] = []\n",
        "        id_to_ranks[webpage_id].append(entry)\n"
      ],
      "metadata": {
        "id": "GSbiSIBFgZ71"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add data to jsons"
      ],
      "metadata": {
        "id": "NcOpiJHvhR6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_folder = \"./com_links\"\n",
        "\n",
        "for webpage_id, rank_entries in id_to_ranks.items():\n",
        "    json_path = os.path.join(json_folder, f\"{webpage_id}.json\")\n",
        "\n",
        "    if not os.path.exists(json_path):\n",
        "        print(f\"Warning: JSON file {json_path} does not exist, skipping.\")\n",
        "        continue\n",
        "\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as jf:\n",
        "        try:\n",
        "            data = json.load(jf)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Warning: Could not decode JSON in {json_path}, skipping.\")\n",
        "            continue\n",
        "\n",
        "    # Add or update \"result_rank\"\n",
        "    if \"result_rank\" not in data:\n",
        "        data[\"result_rank\"] = []\n",
        "    data[\"result_rank\"].extend(rank_entries)\n",
        "\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "        json.dump(data, jf, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ All matching JSON files updated!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfe0VVP1hVHq",
        "outputId": "c6c80199-e035-4df3-b79a-a68c529f0c23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All matching JSON files updated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Scraping"
      ],
      "metadata": {
        "id": "l-gLfb2CmGWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGhtWzj2mJFb",
        "outputId": "05b1a263-7e3f-4a9b-8928-51698bb6a744"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import chardet\n",
        "import time"
      ],
      "metadata": {
        "id": "7XvuCVZnmNE7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "ydqNoTEWmRe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = 'com_links'"
      ],
      "metadata": {
        "id": "-lzHPawdmTSS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = 'com_links_scraped'\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "5ywoMljxm433"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data collection section"
      ],
      "metadata": {
        "id": "wlt9nphcm6JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Starting scraping... Total files: {len(os.listdir(input_folder))}\")\n",
        "\n",
        "for idx, filename in enumerate(os.listdir(input_folder), 1):\n",
        "    if filename.endswith('.json'):\n",
        "        input_filepath = os.path.join(input_folder, filename)\n",
        "        output_filepath = os.path.join(output_folder, filename)\n",
        "\n",
        "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        url = data.get('URL') or data.get('url')\n",
        "        if not url:\n",
        "            print(f\"[{idx}] {filename}: No URL found, skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Skip PDFs ---\n",
        "        if url.lower().endswith('.pdf'):\n",
        "            print(f\"[{idx}] {filename}: URL is a PDF, skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n[{idx}] Processing: {filename}\")\n",
        "        print(f\"    URL: {url}\")\n",
        "\n",
        "        try:\n",
        "            # --- Track timing ---\n",
        "            start_time = time.time()\n",
        "\n",
        "            # GET request\n",
        "            response = requests.get(url, timeout=20, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            download_time = time.time() - start_time\n",
        "\n",
        "            # --- Encoding detection ---\n",
        "            detected = chardet.detect(response.content)\n",
        "            encoding = detected['encoding'] if detected['confidence'] > 0.5 else 'utf-8'\n",
        "            print(f\"    Downloaded in {download_time:.2f}s | Encoding: {encoding}\")\n",
        "\n",
        "            # --- Parsing ---\n",
        "            soup = BeautifulSoup(response.content, 'html.parser', from_encoding=encoding)\n",
        "\n",
        "            # --- Extract fields ---\n",
        "            title = soup.title.string.strip() if soup.title else None\n",
        "\n",
        "            pub_date = None\n",
        "            pub_date_tag = soup.find('meta', property='article:published_time') or soup.find('meta', attrs={'name': 'pubdate'})\n",
        "            if pub_date_tag and pub_date_tag.get('content'):\n",
        "                pub_date = pub_date_tag['content'].strip()\n",
        "\n",
        "            # Main text\n",
        "            paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
        "            headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3'])]\n",
        "            main_text = \"\\n\".join(headings + paragraphs)\n",
        "\n",
        "            # Embedded media\n",
        "            embedded_media = []\n",
        "            for iframe in soup.find_all('iframe', src=True):\n",
        "                embedded_media.append(iframe['src'])\n",
        "            for video in soup.find_all('video', src=True):\n",
        "                embedded_media.append(video['src'])\n",
        "\n",
        "            # Add website_data\n",
        "            data['website_data'] = {\n",
        "                'webpage_title': title,\n",
        "                'publication_date': pub_date,\n",
        "                'main_text': main_text[:10000],\n",
        "                'embedded_media': embedded_media\n",
        "            }\n",
        "\n",
        "            # Save updated JSON\n",
        "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            print(f\"    ✅ Completed and saved to '{output_folder}/{filename}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ Failed to fetch {url}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Scraping complete! JSONs saved in '{output_folder}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgUtyEfym9zV",
        "outputId": "91030df2-d64d-474e-cb6b-1289370158e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scraping... Total files: 81\n",
            "\n",
            "[1] Processing: 3081280474.json\n",
            "    URL: https://www.zgggws.com/article/id/17481?viewType=HTML\n",
            "    Downloaded in 0.43s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/3081280474.json'\n",
            "\n",
            "[2] Processing: 5899489299.json\n",
            "    URL: https://zhuanlan.zhihu.com/p/539664390\n",
            "    Downloaded in 0.39s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5899489299.json'\n",
            "\n",
            "[3] Processing: 7234679320.json\n",
            "    URL: https://news.un.org/zh/story/2019/09/1041062\n",
            "    Downloaded in 2.20s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/7234679320.json'\n",
            "\n",
            "[4] Processing: 3129068608.json\n",
            "    URL: https://www.baidu.com/ssid=c86adcb0/from=844b/s?word=%E6%80%8E%E4%B9%88%E8%87%AA%E6%9D%80&ts=7718745&t_kt=0&ie=utf-8&fm_kl=021394be2f&rsv_iqid=2195213036-6&rsv_t=7ef1MwY6IPilTkA%252FcWv8ZDgshr2Im58iJ4sqPBjODrBI%252BbZ8vLGoZsuAmw&sa=ib&ms=1&rsv_pq=2195213036&rsv_sug4=27874&ss=101&inputT=26056\n",
            "    Downloaded in 1.17s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/3129068608.json'\n",
            "\n",
            "[5] Processing: 0314968668.json\n",
            "    URL: https://m.iask.sina.com.cn/b/1SSTprcUntw7.html\n",
            "    Downloaded in 1.20s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0314968668.json'\n",
            "\n",
            "[6] Processing: 0604907105.json\n",
            "    URL: https://www.judiciary.hk/zh/court_services_facilities/cor.html\n",
            "    Downloaded in 0.40s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0604907105.json'\n",
            "\n",
            "[7] Processing: 1512327726.json\n",
            "    URL: https://www-reangel-com.webpkgcache.com/doc/-/s/www.reangel.com/Article.php?LI=508\n",
            "    Downloaded in 0.01s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/1512327726.json'\n",
            "[8] 7695045207.json: URL is a PDF, skipping.\n",
            "\n",
            "[9] Processing: 3618553785.json\n",
            "    URL: https://www.baidu.com/from=844b/ssid=0/s?word=%E5%8D%81%E5%A4%A7%E5%AE%89%E9%9D%99%E6%AD%BB%E6%B3%95%E6%B2%A1%E4%BA%BA%E5%8F%91%E7%8E%B0&sa=re_dl_prs_34689_1&ms=1&rqid=7615189893333465228&rq=%E6%8A%91%E9%83%81%E7%97%87%E7%9A%84100%E7%A7%8D%E6%AD%BB%E6%B3%95&rsf=1630015&asctag=48041\n",
            "    Downloaded in 0.80s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/3618553785.json'\n",
            "\n",
            "[10] Processing: 0802796350.json\n",
            "    URL: https://m.baidu.com/bh/m/detail/sv_7610000187857257858\n",
            "    Downloaded in 0.70s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0802796350.json'\n",
            "\n",
            "[11] Processing: 4533892844.json\n",
            "    URL: http://www.baidu.com/from=844b/ssid=0/s?word=%E5%9C%A8%E5%AE%B6%E9%87%8C%E6%9C%80%E7%AE%80%E5%8D%95%E6%AD%BB%E4%BA%A1%E6%96%B9%E6%B3%95&sa=re_dl_prs_34689_1&ms=1&rqid=9213115296118607072&rq=%E6%AD%BB%E4%BA%A1%E6%9C%80%E4%BD%B3%E6%96%B9%E5%BC%8F&rsf=1630013&asctag=16508\n",
            "    Downloaded in 1.01s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4533892844.json'\n",
            "\n",
            "[12] Processing: 8100522528.json\n",
            "    URL: https://www.who.int/zh/news-room/fact-sheets/detail/suicide\n",
            "    Downloaded in 0.06s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/8100522528.json'\n",
            "\n",
            "[13] Processing: 8812631163.json\n",
            "    URL: https://bbs.comefromchina.com/threads/34289/\n",
            "    Downloaded in 1.05s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/8812631163.json'\n",
            "\n",
            "[14] Processing: 5622634388.json\n",
            "    URL: http://www.360doc.com/content/17/0311/17/14106735_635857084.shtml\n",
            "    Downloaded in 5.18s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5622634388.json'\n",
            "\n",
            "[15] Processing: 0711786849.json\n",
            "    URL: https://m.baidu.com/bh/m/detail/ar_7130771932829825641\n",
            "    Downloaded in 0.75s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0711786849.json'\n",
            "\n",
            "[16] Processing: 4572251083.json\n",
            "    URL: https://helloyishi.com.tw/sleep/insomnia/sleeping-pills-side-effects/\n",
            "    Downloaded in 0.12s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4572251083.json'\n",
            "\n",
            "[17] Processing: 6649348939.json\n",
            "    URL: https://community.geph.io/t/topic/3847\n",
            "    Downloaded in 1.03s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6649348939.json'\n",
            "\n",
            "[18] Processing: 1487129003.json\n",
            "    URL: https://zh-cn.hsc.unm.edu/medicine/about/well-being/wellness-mental-health-resources/suicide-prevention.html\n",
            "    Downloaded in 1.34s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1487129003.json'\n",
            "\n",
            "[19] Processing: 5010859106.json\n",
            "    URL: https://www.reddit.com/r/China_irl/comments/1ioig4h/%E5%9C%A8%E4%B8%AD%E5%9B%BD%E6%9C%89%E4%BB%80%E4%B9%88%E5%8A%9E%E6%B3%95%E8%83%BD%E5%A4%9F%E5%83%8F%E7%94%A8%E6%9E%AA%E4%B8%80%E6%A0%B7%E8%BF%85%E9%80%9F%E8%87%AA%E6%9D%80%E5%90%97/\n",
            "    Downloaded in 0.09s | Encoding: Windows-1252\n",
            "    ✅ Completed and saved to 'com_links_scraped/5010859106.json'\n",
            "\n",
            "[20] Processing: 6057992841.json\n",
            "    URL: https://news.sina.cn/2014-11-09/detail-icfkptvx4457828.d.html\n",
            "    Downloaded in 3.15s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6057992841.json'\n",
            "\n",
            "[21] Processing: 5354538718.json\n",
            "    URL: https://zhidao.baidu.com/index/?word=%E5%A6%82%E4%BD%95%E5%89%B2%E8%85%95%E8%87%AA%E6%9D%80%E4%B8%8D%E4%BC%9A%E6%AD%BB%EF%BC%9F&from=qb&ad_test=&Ycai_hit=281&uid=bd_1472568435_193&step=1\n",
            "    Downloaded in 1.33s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5354538718.json'\n",
            "[22] 7422813570.json: URL is a PDF, skipping.\n",
            "\n",
            "[23] Processing: 5465282865.json\n",
            "    URL: https://www.yiming818.com/7823.html\n",
            "    ❌ Failed to fetch https://www.yiming818.com/7823.html: HTTPSConnectionPool(host='www.yiming818.com', port=443): Read timed out. (read timeout=20)\n",
            "\n",
            "[24] Processing: 4151506125.json\n",
            "    URL: https://jsjc.gov.cn/shzs/fzzc/202308/t20230808_1537884.shtml\n",
            "    Downloaded in 0.99s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4151506125.json'\n",
            "\n",
            "[25] Processing: 6974964941.json\n",
            "    URL: https://zhidao.baidu.com/index/?word=%E4%B8%80%E8%88%AC%E4%BA%BA%E8%B7%B3%E6%A5%BC%E8%87%AA%E6%9D%80%EF%BC%8C%E4%BB%8E%E5%87%A0%E6%A5%BC%E8%B7%B3%E4%B8%8B%E5%8E%BB%E8%82%AF%E5%AE%9A%E4%BC%9A%E6%AD%BB&from=qb&ad_test=&Ycai_hit=253&uid=C3A971661783E0112202CE876D300263&step=2\n",
            "    Downloaded in 0.74s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6974964941.json'\n",
            "[26] 8197407423.json: URL is a PDF, skipping.\n",
            "\n",
            "[27] Processing: 5152876258.json\n",
            "    URL: https://m.ximalaya.com/ask/q3993003\n",
            "    Downloaded in 0.61s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5152876258.json'\n",
            "\n",
            "[28] Processing: 1114413690.json\n",
            "    URL: https://www.faxin.cn/lib/Flwx/FlqkContent.aspx?gid=F56763&libid=040101\n",
            "    Downloaded in 1.41s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1114413690.json'\n",
            "\n",
            "[29] Processing: 9467082684.json\n",
            "    URL: https://www.commonhealth.com.tw/article/82568\n",
            "    Downloaded in 0.06s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/9467082684.json'\n",
            "\n",
            "[30] Processing: 6877497266.json\n",
            "    URL: https://www.twreporter.org/a/elementary-students-suicide\n",
            "    Downloaded in 0.15s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6877497266.json'\n",
            "\n",
            "[31] Processing: 5740768700.json\n",
            "    URL: https://zh.wikipedia.org/zh-hans/%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F\n",
            "    Downloaded in 1.68s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5740768700.json'\n",
            "\n",
            "[32] Processing: 8750036588.json\n",
            "    URL: https://www.msdmanuals.cn/home/mental-health-disorders/suicidal-behavior-and-self-injury/suicidal-behavior\n",
            "    ❌ Failed to fetch https://www.msdmanuals.cn/home/mental-health-disorders/suicidal-behavior-and-self-injury/suicidal-behavior: HTTPSConnectionPool(host='www.msdmanuals.cn', port=443): Max retries exceeded with url: /home/mental-health-disorders/suicidal-behavior-and-self-injury/suicidal-behavior (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7b2b396df7d0>, 'Connection to www.msdmanuals.cn timed out. (connect timeout=20)'))\n",
            "\n",
            "[33] Processing: 1885847504.json\n",
            "    URL: http://www.wellyclinic.tw/h/ServiceDetail?key=3501106059&cont=2503&set=7\n",
            "    Downloaded in 0.57s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1885847504.json'\n",
            "\n",
            "[34] Processing: 4903961193.json\n",
            "    URL: https://www.mghclaycenter.org/parenting-concerns/%E9%A2%84%E9%98%B2%E8%87%AA%E6%9D%80%E5%A6%82%E4%BD%95%E5%B8%AE%E5%8A%A9%E6%82%A8%E7%9A%84%E5%AD%A9%E5%AD%90/\n",
            "    Downloaded in 0.90s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4903961193.json'\n",
            "\n",
            "[35] Processing: 5640325990.json\n",
            "    URL: https://www.city.kokubunji.tokyo.jp.c.amu.hp.transer.com/kurashi/1008592/1012510/1028978.html\n",
            "    Downloaded in 0.08s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/5640325990.json'\n",
            "\n",
            "[36] Processing: 3435152166.json\n",
            "    URL: https://www.who.int/zh/news/item/09-09-2019-suicide-one-person-dies-every-40-seconds\n",
            "    Downloaded in 0.46s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/3435152166.json'\n",
            "\n",
            "[37] Processing: 9968944052.json\n",
            "    URL: https://zhidao.baidu.com/question/51172329.html\n",
            "    ❌ Failed to fetch https://zhidao.baidu.com/question/51172329.html: HTTPSConnectionPool(host='zhidao.baidu.com', port=443): Max retries exceeded with url: /question/51172329.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7b2b3934f7d0>, 'Connection to zhidao.baidu.com timed out. (connect timeout=20)'))\n",
            "\n",
            "[38] Processing: 8977998254.json\n",
            "    URL: https://vocus.cc/article/656e9532fd89780001ae2600\n",
            "    Downloaded in 0.14s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/8977998254.json'\n",
            "[39] 0234222609.json: URL is a PDF, skipping.\n",
            "\n",
            "[40] Processing: 0424515202.json\n",
            "    URL: https://wwwv.tsgh.ndmctsgh.edu.tw/unit/10019/18035\n",
            "    Downloaded in 0.04s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/0424515202.json'\n",
            "\n",
            "[41] Processing: 6274604123.json\n",
            "    URL: https://www.facebook.com/SinChewDaily/posts/%E6%98%A8%E5%8D%88%E4%BB%8E%E5%AD%A6%E6%A0%A13%E6%A5%BC%E5%9D%A0%E6%A5%BC%E7%9A%8413%E5%B2%81%E4%B8%AD%E4%B8%80%E5%AD%A6%E7%94%9F%E5%B7%B2%E4%BA%8E%E4%BB%8A%E6%97%A5%E5%87%8C%E6%99%A812%E6%97%B630%E5%88%86%E5%B7%A6%E5%8F%B3%E9%80%9D%E4%B8%96%E6%98%9F%E6%B4%B2%E6%97%A5%E6%8A%A5-sinchew/873269434834585/\n",
            "    Downloaded in 0.80s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6274604123.json'\n",
            "[42] 2074393071.json: URL is a PDF, skipping.\n",
            "[43] 6680502017.json: URL is a PDF, skipping.\n",
            "\n",
            "[44] Processing: 4969809966.json\n",
            "    URL: https://sbhk.org.hk/?page_id=34172\n",
            "    Downloaded in 0.86s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4969809966.json'\n",
            "\n",
            "[45] Processing: 0287215483.json\n",
            "    URL: https://www.52hrtt.com/ylsn/n/w/info/G1688457679888\n",
            "    Downloaded in 1.25s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0287215483.json'\n",
            "\n",
            "[46] Processing: 1178982918.json\n",
            "    URL: https://m.baidu.com/bh/m/detail/qr_12537908863696279478\n",
            "    Downloaded in 0.54s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1178982918.json'\n",
            "\n",
            "[47] Processing: 7235673417.json\n",
            "    URL: https://www.clic.org.hk/zh/topics/employmentDisputes/otherMajorEmploymentAndLabourLegislation/occupationalSafetyAndHealthOrdinance/obligations_in_workplace_accidents\n",
            "    Downloaded in 0.51s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/7235673417.json'\n",
            "\n",
            "[48] Processing: 0523169920.json\n",
            "    URL: https://m.ac.qq.com/Ask/detail/aid/petuosrhgt\n",
            "    Downloaded in 0.27s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0523169920.json'\n",
            "\n",
            "[49] Processing: 1814451065.json\n",
            "    URL: http://www.baidu.com/from=844b/ssid=0/s?word=%E5%8D%81%E5%A4%A7%E5%BF%AB%E9%80%9F%E6%AD%BB%E4%BA%A1%E6%97%A0%E7%97%9B%E6%96%B9%E6%B3%95&sa=re_dl_prs_34689_5&ms=1&rqid=11657973654154055419&rq=%E7%9D%A1%E8%A7%89%E6%AD%BB%E4%BA%A1%E6%B3%95&rsf=1630013&asctag=58000\n",
            "    Downloaded in 0.94s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1814451065.json'\n",
            "\n",
            "[50] Processing: 7892482169.json\n",
            "    URL: http://www.lzkzsf.com/wzxq?article_id=9\n",
            "    Downloaded in 2.60s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/7892482169.json'\n",
            "\n",
            "[51] Processing: 7375218039.json\n",
            "    URL: https://drkuo-clinic.com.tw/medical_information/sleeping-pills-overdose/\n",
            "    Downloaded in 2.25s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/7375218039.json'\n",
            "\n",
            "[52] Processing: 8609037987.json\n",
            "    URL: https://zh.wikipedia.org/zh-cn/%E5%89%B2%E8%85%95\n",
            "    Downloaded in 0.90s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/8609037987.json'\n",
            "\n",
            "[53] Processing: 6367802363.json\n",
            "    URL: https://www.baidu.com/from=844b/ssid=0/s?word=%E5%9C%A8%E5%AE%B6%E9%87%8C%E6%9C%80%E7%AE%80%E5%8D%95%E6%AD%BB%E4%BA%A1%E6%96%B9%E6%B3%95&sa=re_dl_prs_34689_1&ms=1&rqid=9213115296118607072&rq=%E6%AD%BB%E4%BA%A1%E6%9C%80%E4%BD%B3%E6%96%B9%E5%BC%8F&rsf=1630013&asctag=16508\n",
            "    Downloaded in 3.71s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6367802363.json'\n",
            "\n",
            "[54] Processing: 1058899721.json\n",
            "    URL: https://www.mayoclinic.org/zh-hans/healthy-lifestyle/end-of-life/in-depth/suicide/art-20044900\n",
            "    Downloaded in 0.24s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1058899721.json'\n",
            "\n",
            "[55] Processing: 6649034752.json\n",
            "    URL: https://news.qq.com/rain/a/20200109A0766K00\n",
            "    Downloaded in 0.20s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6649034752.json'\n",
            "\n",
            "[56] Processing: 2939355824.json\n",
            "    URL: https://www.baidu.com/from=844b/ssid=5a4c57494e47544f4e4738387098/s?word=%E5%8D%81%E5%A4%A7%E5%BF%AB%E9%80%9F%E6%AD%BB%E4%BA%A1%E6%97%A0%E7%97%9B%E6%96%B9%E6%B3%95&sa=re_dl_prs_34689_4&ms=1&rqid=9046072404671267887&rq=%E7%A7%91%E5%AD%A6%E8%A7%A3%E9%87%8A%E6%AD%BB%E4%BA%A1%E4%B8%8D%E5%8F%AF%E6%80%95&rsf=1630001&asctag=38784\n",
            "    Downloaded in 1.06s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/2939355824.json'\n",
            "\n",
            "[57] Processing: 6588815577.json\n",
            "    URL: https://www.zhihu.com/question/33627410\n",
            "    Downloaded in 0.56s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6588815577.json'\n",
            "\n",
            "[58] Processing: 4261262117.json\n",
            "    URL: https://zh.wikipedia.org/wiki/%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F\n",
            "    Downloaded in 1.64s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4261262117.json'\n",
            "\n",
            "[59] Processing: 3905811891.json\n",
            "    URL: https://www.tact4brain.com/l/%E6%88%91%E6%83%B3%E8%87%AA%E6%AE%BA/\n",
            "    Downloaded in 1.10s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/3905811891.json'\n",
            "\n",
            "[60] Processing: 1513850096.json\n",
            "    URL: https://www.typc.mohw.gov.tw/?aid=512&pid=42&page_name=detail&iid=150\n",
            "    Downloaded in 3.40s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1513850096.json'\n",
            "\n",
            "[61] Processing: 9702304018.json\n",
            "    URL: https://m.gmw.cn/2023-06/20/content_1303411990.htm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Downloaded in 1.50s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/9702304018.json'\n",
            "[62] 5330054929.json: URL is a PDF, skipping.\n",
            "\n",
            "[63] Processing: 4621844064.json\n",
            "    URL: https://zh.wikipedia.org/zh-hk/%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F\n",
            "    Downloaded in 1.69s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4621844064.json'\n",
            "\n",
            "[64] Processing: 6889589666.json\n",
            "    URL: http://www.chinapeace.gov.cn/chinapeace/c100007/2021-08/23/content_12527450.shtml\n",
            "    Downloaded in 0.50s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6889589666.json'\n",
            "\n",
            "[65] Processing: 1206003000.json\n",
            "    URL: https://www.pkuh6.cn/Html/News/Articles/3642.html\n",
            "    Downloaded in 5.02s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/1206003000.json'\n",
            "\n",
            "[66] Processing: 6060248314.json\n",
            "    URL: https://www.dcard.tw/f/mood/p/235861918\n",
            "    Downloaded in 0.04s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/6060248314.json'\n",
            "\n",
            "[67] Processing: 5619025037.json\n",
            "    URL: https://nsa-sleep.com/archives/2502\n",
            "    Downloaded in 0.18s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5619025037.json'\n",
            "\n",
            "[68] Processing: 0431867372.json\n",
            "    URL: http://www.baidu.com/from=844b/ssid=0/s?word=%E5%8D%81%E7%A7%8D%E5%AE%89%E9%9D%99%E7%9A%84%E6%AD%BB&sa=re_dl_prs_34689_4&ms=1&rqid=11415080424556571509&rq=%E6%9C%8D%E7%94%A8%E6%B0%B0%E5%8C%96%E9%92%A0%E4%B8%80%E5%85%8B%E5%A4%9A%E4%B9%85%E6%AD%BB&rsf=1630001&asctag=44744\n",
            "    Downloaded in 14.73s | Encoding: MacRoman\n",
            "    ✅ Completed and saved to 'com_links_scraped/0431867372.json'\n",
            "\n",
            "[69] Processing: 2072530746.json\n",
            "    URL: https://www.thepaper.cn/newsDetail_forward_20608639\n",
            "    Downloaded in 0.82s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/2072530746.json'\n",
            "\n",
            "[70] Processing: 5702703508.json\n",
            "    URL: https://zhuanlan.zhihu.com/p/89450837\n",
            "    Downloaded in 0.53s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5702703508.json'\n",
            "\n",
            "[71] Processing: 4642894593.json\n",
            "    URL: https://health.businessweekly.com.tw/article/ARTL000065510\n",
            "    Downloaded in 0.10s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4642894593.json'\n",
            "\n",
            "[72] Processing: 6034589709.json\n",
            "    URL: https://pincong.rocks/question/56133\n",
            "    Downloaded in 0.08s | Encoding: ascii\n",
            "    ✅ Completed and saved to 'com_links_scraped/6034589709.json'\n",
            "\n",
            "[73] Processing: 4616083969.json\n",
            "    URL: https://zh.wikipedia.org/zh-cn/%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F\n",
            "    Downloaded in 1.30s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/4616083969.json'\n",
            "\n",
            "[74] Processing: 5205659674.json\n",
            "    URL: https://m.ximalaya.com/ask/t6481497\n",
            "    Downloaded in 2.83s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5205659674.json'\n",
            "\n",
            "[75] Processing: 5917712145.json\n",
            "    URL: https://zhuanlan.zhihu.com/p/354495847\n",
            "    Downloaded in 0.48s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5917712145.json'\n",
            "\n",
            "[76] Processing: 1370427530.json\n",
            "    URL: https://www.163.com/dy/article/GGFK5IMO05369SE2.html\n",
            "    Downloaded in 0.45s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/1370427530.json'\n",
            "[77] 1249805490.json: URL is a PDF, skipping.\n",
            "\n",
            "[78] Processing: 0042714234.json\n",
            "    URL: https://www.cochrane.org/zh-hans/CD013738/DEPRESSN_fang-zhi-dao-lu-zi-sha-de-xian-zhi-fang-fa\n",
            "    Downloaded in 1.03s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/0042714234.json'\n",
            "\n",
            "[79] Processing: 6965735248.json\n",
            "    URL: https://www.farhugs.com/questions/3198\n",
            "    Downloaded in 0.30s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/6965735248.json'\n",
            "\n",
            "[80] Processing: 9912012442.json\n",
            "    URL: https://www.mayoclinic.org/zh-hans/diseases-conditions/suicide/in-depth/suicide/art-20048230\n",
            "    Downloaded in 2.35s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/9912012442.json'\n",
            "\n",
            "[81] Processing: 5225866086.json\n",
            "    URL: https://zh.wikipedia.org/zh-hans/%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F#:~:text=%E8%87%AA%E6%9D%80%E6%96%B9%E5%BC%8F%E6%98%AF%E6%8C%87%E4%B8%80%E4%B8%AA,%E5%8F%8A%E6%B3%A8%E5%B0%84%E8%BF%87%E9%87%8F%E8%8D%AF%E7%89%A9%E7%AD%89%E3%80%82&text=%E8%87%AA%E6%9D%80%E6%9C%AA%E9%81%82%E7%9A%84%E4%BA%BA%E5%8F%AF%E8%83%BD,%E5%81%A5%E5%BA%B7%E5%8F%AF%E6%9C%89%E9%95%BF%E6%9C%9F%E5%BD%B1%E5%93%8D%E3%80%82\n",
            "    Downloaded in 0.29s | Encoding: utf-8\n",
            "    ✅ Completed and saved to 'com_links_scraped/5225866086.json'\n",
            "\n",
            "✅ Scraping complete! JSONs saved in 'com_links_scraped'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleanup -- lots of websites not detected by Beautifulsoup"
      ],
      "metadata": {
        "id": "g5_F5UoZrefT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your folders\n",
        "com_links_folder = 'com_links'\n",
        "scraped_folder = 'com_links_scraped'\n",
        "\n",
        "# Get list of filenames in each folder\n",
        "com_links_files = set(os.listdir(com_links_folder))\n",
        "scraped_files = set(os.listdir(scraped_folder))\n",
        "\n",
        "# Find files in com_links not in scraped\n",
        "missing_files = com_links_files - scraped_files\n",
        "\n",
        "# Output\n",
        "print(f\"Total files in '{com_links_folder}': {len(com_links_files)}\")\n",
        "print(f\"Total files in '{scraped_folder}': {len(scraped_files)}\")\n",
        "print(f\"\\nFiles not yet scraped ({len(missing_files)}):\")\n",
        "\n",
        "for filename in sorted(missing_files):\n",
        "    print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1cup7a-xwKE",
        "outputId": "a54aaa0c-6c0c-4b6b-ad11-3d60b96eca22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files in 'com_links': 81\n",
            "Total files in 'com_links_scraped': 70\n",
            "\n",
            "Files not yet scraped (11):\n",
            "0234222609.json\n",
            "1249805490.json\n",
            "2074393071.json\n",
            "5330054929.json\n",
            "5465282865.json\n",
            "6680502017.json\n",
            "7422813570.json\n",
            "7695045207.json\n",
            "8197407423.json\n",
            "8750036588.json\n",
            "9968944052.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Need to add manual scraping into the jsons"
      ],
      "metadata": {
        "id": "oDz1syY_0i6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "GT-nKjD_00fS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "com_links_folder = 'com_links'\n",
        "scraped_folder = 'com_links_scraped'"
      ],
      "metadata": {
        "id": "YK1RXfoM03F1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "excel_file2 = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(excel_file2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "CqcB4OX20_lT",
        "outputId": "a7a0eb3f-3c24-4b91-f25b-13f339e16aa2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36788a16-d639-4806-b0c0-6b9976fce43b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36788a16-d639-4806-b0c0-6b9976fce43b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving manual_scrape.xlsx to manual_scrape.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded Excel with {len(df)} rows\")\n",
        "\n",
        "# Process each row\n",
        "for idx, row in df.iterrows():\n",
        "    file_id = str(row['ID']).strip()\n",
        "    page_title = str(row['Page Title']).strip() if pd.notnull(row['Page Title']) else None\n",
        "    page_body = str(row['Page Body']).strip() if pd.notnull(row['Page Body']) else None\n",
        "\n",
        "    filename = f\"{file_id}.json\"\n",
        "    input_path = os.path.join(com_links_folder, filename)\n",
        "    output_path = os.path.join(scraped_folder, filename)\n",
        "\n",
        "    # Check if exists in com_links folder\n",
        "    if os.path.exists(input_path):\n",
        "        print(f\"[{idx+1}] Processing {filename}\")\n",
        "\n",
        "        # Load JSON\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Add website_data\n",
        "        data['website_data'] = {\n",
        "            'webpage_title': page_title,\n",
        "            'publication_date': None,\n",
        "            'main_text': page_body[:10000] if page_body else '',\n",
        "            'embedded_media': []\n",
        "        }\n",
        "\n",
        "        # Delete existing file in scraped folder if exists\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "            print(f\"    Existing file in scraped folder removed: {output_path}\")\n",
        "\n",
        "        # Save updated JSON to scraped folder\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"    ✅ Saved updated JSON to: {output_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"[{idx+1}] File {filename} not found in '{com_links_folder}', skipping.\")\n",
        "\n",
        "print(\"\\n✅ All matching files processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5monXMP1WFk",
        "outputId": "875dd08a-2c0a-4933-caac-844dd774d527"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Excel with 27 rows\n",
            "[1] Processing 3081280474.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/3081280474.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/3081280474.json\n",
            "[2] Processing 1512327726.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/1512327726.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/1512327726.json\n",
            "[3] Processing 7695045207.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/7695045207.json\n",
            "[4] Processing 8812631163.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/8812631163.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/8812631163.json\n",
            "[5] Processing 5010859106.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/5010859106.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5010859106.json\n",
            "[6] Processing 7422813570.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/7422813570.json\n",
            "[7] Processing 8197407423.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/8197407423.json\n",
            "[8] Processing 5152876258.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/5152876258.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5152876258.json\n",
            "[9] Processing 9467082684.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/9467082684.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/9467082684.json\n",
            "[10] Processing 5640325990.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/5640325990.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5640325990.json\n",
            "[11] Processing 9968944052.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/9968944052.json\n",
            "[12] File 234222609.json not found in 'com_links', skipping.\n",
            "[13] Processing 6274604123.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/6274604123.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/6274604123.json\n",
            "[14] Processing 2074393071.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/2074393071.json\n",
            "[15] Processing 6680502017.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/6680502017.json\n",
            "[16] Processing 1178982918.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/1178982918.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/1178982918.json\n",
            "[17] Processing 6649034752.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/6649034752.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/6649034752.json\n",
            "[18] Processing 1513850096.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/1513850096.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/1513850096.json\n",
            "[19] Processing 5330054929.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5330054929.json\n",
            "[20] Processing 1206003000.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/1206003000.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/1206003000.json\n",
            "[21] Processing 6034589709.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/6034589709.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/6034589709.json\n",
            "[22] Processing 5205659674.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/5205659674.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5205659674.json\n",
            "[23] Processing 1249805490.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/1249805490.json\n",
            "[24] Processing 5354538718.json\n",
            "    Existing file in scraped folder removed: com_links_scraped/5354538718.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5354538718.json\n",
            "[25] File 431867372.json not found in 'com_links', skipping.\n",
            "[26] Processing 8750036588.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/8750036588.json\n",
            "[27] Processing 5465282865.json\n",
            "    ✅ Saved updated JSON to: com_links_scraped/5465282865.json\n",
            "\n",
            "✅ All matching files processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Cleaned Data"
      ],
      "metadata": {
        "id": "1jNinHTD116m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_to_zip = 'com_links_scraped'\n",
        "output_zip = 'com_links_scraped.zip'\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive('com_links_scraped', 'zip', folder_to_zip)\n",
        "print(f\"✅ Folder '{folder_to_zip}' zipped as '{output_zip}'\")\n",
        "\n",
        "# Download zip file\n",
        "files.download(output_zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z8COtf5215aN",
        "outputId": "20e712a8-37f1-4d2d-e3db-31a9b3a112ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Folder 'com_links_scraped' zipped as 'com_links_scraped.zip'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_996227db-ebfb-4b15-b451-57abd3828934\", \"com_links_scraped.zip\", 294184)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}